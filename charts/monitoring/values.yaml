global:
  storageClassName: ~
  domain: 'grafana.example.team'

## @param kubeVersion Override Kubernetes version
##
kubeVersion: ""
## @param nameOverride String to partially override common.names.fullname template (will maintain the release name)
##
nameOverride: ""
## @param fullnameOverride String to fully override common.names.fullname template
##
fullnameOverride: ""
## @param commonLabels Labels to add to all deployed resources
##
commonLabels: { }

commonAnnotations:
  "kubernetes.io/description": '{{ include "common.names.fullname" $ }} monitoring'

##
##
storageClasses:
  '{{ include "common.storage-className" $ }}':
    ##
    ##
    enabled: true
    ##
    ##
    additionalAnnotations: { }
    ##
    ##
    additionalLabels: { }
    ##
    ##
    provisioner: ""
    ##
    ##
    parameters: { }
    ##
    ##
    volumeBindingMode: ""
    ##
    ##
    reclaimPolicy: Delete

keycloak:
  enabled: true
  client_id: "grafana"
  client_secret: "some-secret"
  url: "https://keycloak.example.me"
  realm: "demo"

prom-stack:
  alertmanager:
    ingress:
      enabled: false
      ingressClassName: traefik
      hosts:
        - "alertmanager-{{ .Values.global.domain }}"
    alertmanagerSpec:
      logLevel: error
      storage:
        volumeClaimTemplate:
          spec:
            storageClassName: '{{ include "common.storage-className" $ }}'
            accessModes: [ "ReadWriteOnce" ]
            resources:
              requests:
                storage: 10Gi

  prometheus:
    ingress:
      enabled: false
      ingressClassName: traefik
      hosts:
        - "prometheus-{{ .Values.global.domain }}"
    prometheusSpec:
      retention: 10d
      #logLevel: error
      #      securityContext:
      #        runAsGroup: 0
      #        runAsNonRoot: false
      #        runAsUser: 0
      #        fsGroup: 0
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: '{{ include "common.storage-className" $ }}'
            accessModes: [ "ReadWriteOnce" ]
            resources:
              requests:
                storage: 10Gi

  grafana:
    ingress:
      enabled: true
      ingressClassName: traefik
      hosts:
        - "{{ .Values.global.domain }}"
    sidecar:
      datasources:
        enabled: true
        label: grafana_datasource
        isDefaultDatasource: false
        defaultDatasourceEnabled: false
    plugins:
      - grafana-piechart-panel
      - grafana-clock-panel
      - digrich-bubblechart-panel
    enabled: true
    envFromSecret: '{{ $.Release.Name }}-keycloak-conf'
    grafana.ini:
      server:
        root_url: "https://{{ .Values.global.domain }}"
        enable_gzip: true
      auth:
        disable_login_form: true
      auth.anonymous:
        enabled: "false"
        org_role: "Admin"
      auth.basic:
        enabled: "false"
      auth.generic_oauth:
        enabled: "true"
        name: "Keycloak login"
        auto_login: "true"
        allow_sign_up: "true"
        client_id: '$__env{KEYCLOAK_CLIENT_ID}'
        client_secret: '$__env{KEYCLOAK_CLIENT_SECRET}'
        scopes: "openid email profile offline_access roles"
        email_attribute_path: "email"
        login_attribute_path: "username"
        name_attribute_path: "full_name"
        auth_url: "$__env{KEYCLOAK_ISSUER}/protocol/openid-connect/auth"
        token_url: "$__env{KEYCLOAK_ISSUER}/protocol/openid-connect/token"
        api_url: "$__env{KEYCLOAK_ISSUER}/protocol/openid-connect/userinfo"
        signout_redirect_url: "$__env{KEYCLOAK_ISSUER}/protocol/openid-connect/logout?post_logout_redirect_uri=https%3A%2F%2F{{ .Values.global.domain }}%2Flogin"
        role_attribute_path: "contains(grafana_roles[*], 'grafanaadmin') && 'GrafanaAdmin' || contains(grafana_roles[*], 'admin') && 'Admin' || contains(grafana_roles[*], 'editor') && 'Editor' || contains(grafana_roles[*], 'viewer') && 'Viewer' || 'None'"
        allow_assign_grafana_admin: "true"
        use_refresh_token: "true"
        use_pkce: "true"

loki:
  fullnameOverride: loki
  enabled: true
  memberlist:
    service:
      publishNotReadyAddresses: true
  ingress:
    enabled: false
    ingressClassName: traefik
    hosts:
      - "loki-{{ .Values.global.domain }}"
  loki:
    auth_enabled: false
    storage:
      type: s3
      bucketNames:
        chunks: monitoring
        ruler: monitoring
      object_store:
        storage_prefix: "loki_"
    structuredConfig:
      common:
        storage:
          s3:
            #endpoint: "<your-s3-endpoint>"
            access_key_id: "<your-s3-access-key-id>"
            secret_access_key: "<your-s3-secret-access-key>"
      compactor:
        retention_enabled: true
        delete_request_store: s3
      limits_config:
        retention_period: 30d
    
    commonConfig:
      replication_factor: 1
    schemaConfig:
      configs:
        - from: "2024-04-01"
          store: tsdb
          object_store: s3
          schema: v13
          index:
            prefix: loki_index_
            period: 24h
    pattern_ingester:
      enabled: true
    limits_config:
      allow_structured_metadata: true
      volume_enabled: true
  ruler:
    enable_api: true
  lokiCanary:
    enabled: false
  test:
    enabled: false
  monitoring:
    dashboards:
      enabled: false
    rules:
      enabled: false
    serviceMonitor:
      enabled: false
    selfMonitoring:
      enabled: false
      grafanaAgent:
        installOperator: false
    lokiCanary:
      enabled: false

  minio:
    enabled: false

  deploymentMode: SingleBinary

  singleBinary:
    replicas: 1
    resources:
      requests:
        cpu: '100m'
        memory: '600Mi'
      limits:
        cpu: '1000m'
        memory: '4Gi'

  # Zero out replica counts of other deployment modes
  backend:
    replicas: 0
  read:
    replicas: 0
  write:
    replicas: 0

  ingester:
    replicas: 0
  querier:
    replicas: 0
  queryFrontend:
    replicas: 0
  queryScheduler:
    replicas: 0
  distributor:
    replicas: 0
  compactor:
    replicas: 0
  indexGateway:
    replicas: 0
  bloomCompactor:
    replicas: 0
  bloomGateway:
    replicas: 0

alloy:
  enabled: true
  ingress:
    enabled: false
    ingressClassName: traefik
    hosts:
      - "alloy-{{ .Values.global.domain }}"
  alloy:
    configMap:
      content: |-
        logging {
          level  = "info"
          format = "logfmt"
        }
        
        discovery.kubernetes "pods" {
          role = "pod"
        }
        
        discovery.kubernetes "nodes" {
          role = "node"
        }
        
        discovery.kubernetes "services" {
          role = "service"
        }
        
        discovery.kubernetes "endpoints" {
          role = "endpoints"
        }
        
        discovery.kubernetes "endpointslices" {
          role = "endpointslice"
        }
        
        discovery.kubernetes "ingresses" {
          role = "ingress"
        }
        
        loki.write "default" {
          endpoint {
            url = "http://loki:3100/loki/api/v1/push"
          }
        }
        
        // local.file_match discovers files on the local filesystem using glob patterns and the doublestar library. It returns an array of file paths.
        local.file_match "node_logs" {
          path_targets = [{
            // Monitor syslog to scrape node-logs
            __path__  = "/var/log/syslog",
            job       = "node/syslog",
            node_name = sys.env("HOSTNAME"),
            cluster   = "main",
          }]
        }
        
          // loki.source.file reads log entries from files and forwards them to other loki.* components.
          // You can specify multiple loki.source.file components by giving them different labels.
        loki.source.file "node_logs" {
          targets    = local.file_match.node_logs.targets
          forward_to = [loki.write.default.receiver]
        }
        
        // discovery.kubernetes allows you to find scrape targets from Kubernetes resources.
        // It watches cluster state and ensures targets are continually synced with what is currently running in your cluster.
        discovery.kubernetes "pod" {
          role = "pod"
        }

        // discovery.relabel rewrites the label set of the input targets by applying one or more relabeling rules.
        // If no rules are defined, then the input targets are exported as-is.
        discovery.relabel "pod_logs" {
          targets = discovery.kubernetes.pod.targets

          // Label creation - "namespace" field from "__meta_kubernetes_namespace"
          rule {
            source_labels = ["__meta_kubernetes_namespace"]
            action = "replace"
            target_label = "namespace"
          }

          // Label creation - "pod" field from "__meta_kubernetes_pod_name"
          rule {
            source_labels = ["__meta_kubernetes_pod_name"]
            action = "replace"
            target_label = "pod"
          }

          // Label creation - "container" field from "__meta_kubernetes_pod_container_name"
          rule {
            source_labels = ["__meta_kubernetes_pod_container_name"]
            action = "replace"
            target_label = "container"
          }

          // Label creation -  "app" field from "__meta_kubernetes_pod_label_app_kubernetes_io_name"
          rule {
            source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
            action = "replace"
            target_label = "app"
          }

          // Label creation -  "job" field from "__meta_kubernetes_namespace" and "__meta_kubernetes_pod_container_name"
          // Concatenate values __meta_kubernetes_namespace/__meta_kubernetes_pod_container_name
          rule {
            source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
            action = "replace"
            target_label = "job"
            separator = "/"
            replacement = "$1"
          }

          // Label creation - "container" field from "__meta_kubernetes_pod_uid" and "__meta_kubernetes_pod_container_name"
          // Concatenate values __meta_kubernetes_pod_uid/__meta_kubernetes_pod_container_name.log
          rule {
            source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
            action = "replace"
            target_label = "__path__"
            separator = "/"
            replacement = "/var/log/pods/*$1/*.log"
          }

          // Label creation -  "container_runtime" field from "__meta_kubernetes_pod_container_id"
          rule {
            source_labels = ["__meta_kubernetes_pod_container_id"]
            action = "replace"
            target_label = "container_runtime"
            regex = "^(\\S+):\\/\\/.+$"
            replacement = "$1"
          }
        }

        // loki.source.kubernetes tails logs from Kubernetes containers using the Kubernetes API.
        loki.source.kubernetes "pod_logs" {
          targets    = discovery.relabel.pod_logs.output
          forward_to = [loki.process.pod_logs.receiver]
        }

        // loki.process receives log entries from other Loki components, applies one or more processing stages,
        // and forwards the results to the list of receivers in the component's arguments.
        loki.process "pod_logs" {
          stage.static_labels {
              values = {
                cluster = "main",
              }
          }

          forward_to = [loki.write.default.receiver]
        }
        
        // loki.source.kubernetes_events tails events from the Kubernetes API and converts them
        // into log lines to forward to other Loki components.
        loki.source.kubernetes_events "cluster_events" {
          job_name   = "integrations/kubernetes/eventhandler"
          log_format = "logfmt"
          forward_to = [
            loki.process.cluster_events.receiver,
          ]
        }

        // loki.process receives log entries from other loki components, applies one or more processing stages,
        // and forwards the results to the list of receivers in the component's arguments.
        loki.process "cluster_events" {
          forward_to = [loki.write.default.receiver]

          stage.static_labels {
            values = {
              cluster = "main",
            }
          }

          stage.labels {
            values = {
              kubernetes_cluster_events = "job",
            }
          }
        }
    resources:
      requests:
        cpu: '100m'
        memory: '600Mi'
      limits:
        cpu: '800m'
        memory: '4Gi'
    extraPorts:
      - name: "otel"
        port: 4317
        targetPort: 4317
        protocol: "TCP"
      - name: "faro"
        port: 12347
        targetPort: 12347
        protocol: "TCP"
        appProtocol: "h2c"
      - name: "thrifthttp"
        port: 14268
        targetPort: 14268
        protocol: "TCP"